# 室内环境：建图与导航
## 1. SLAM建图 + 自动导航
---
### 1.1 目标

- 用 **激光雷达**（仿真）
- 建立 **3D 地图**
- 保存地图

---
### 1.2 工具

**SLAM算法** : **RTAB-Map(暂定)** 
**仿真** ： Gazebo  

安装：
``` bash
sudo apt install ros-humble-rtabmap-ros
```

---
### 1.3 跑通官方 demo

``` bash
ros2 launch rtabmap_ros demo.launch.py
```
须看到：
- 实时点云
- 3D 地图增长
- 回环检测触发

---
### 1.4 核心数据流

```
LiDAR / RGB-D   #传感器获取数据    
    ↓    
RTAB-Map        #数据处理与融合 建图 定位
    ↓    
#ros2中slam系统发布话题    
/map (3D)  #表示机器人环境的地图，是一个 **静态** 地图（3D 点云或者网格地图），                  记录了机器人所建的环境地图
/odom      #表示机器人的 **位置** 和 **运动轨迹**

TF: map → odom → base_link
```

---
### 1.5 保存地图

``` bash
ros2 run nav2_map_server map_saver_cli -f home_map
```

---
##  2. 定位

> 建图 ≠ 定位  
> 导航必须要定位

---
### 2.1 使用 AMCL / RTAB-Map Localization 模式

- 关闭建图
- 打开定位模式
``` bash
ros2 launch rtabmap_ros localization.launch.py
```

**目标** ： **机器人在 RViz 中能稳定出现在地图正确位置**

---
##  3. 导航

---
### 3.1 使用 ROS2 Nav2

``` bash
sudo apt install ros-humble-navigation2 ros-humble-nav2-bringup
#跑通 Nav2 demo
ros2 launch nav2_bringup navigation_launch.py
```
看到：
- 全局路径（绿色）
- 局部路径（蓝色）
- 自动避障

---
### 3.3 航点保存

**做法**：
#### 记录点
1. 在 RViz 中记录目标位置坐标，例如卧室，厨房等，一般选取中心点或者其他代表性的点位
2. 保存为 YAML
3. 定义一个 **容忍区域**，即到达目标点的周围一定范围内。。如果目标点被占用，机器人可以在 **容忍区域** 内找到一个合适的位置停下，不强制要求到达目标点
#### 记录区域
1. 在 RViz 中记录目标位置坐标，例如卧室，厨房等，记录多个航点来覆盖这个空间范围，或者通过 **多点目标规划** 来描述一个区域
2. 保存为 YAML
3. 机器人在区域内自动寻找空间停下

---
### 3.4 App 点击 → 机器人导航

```
先不用写 App  , **用终端模拟** : 
``` bash
ros2 action send_goal /navigate_to_pose nav2_msgs/action/NavigateToPose "{pose: {...}}"
```

# 室外场景：视觉识别 + 人体跟踪 + 跟随
## 1. YOLO 人体检测

---
### 1.1 目标

- 摄像头输入
- 实时检测 “指定目标（人）”

---
### 1.2 使用 YOLOv8

``` bash
pip install ultralytics
```

简单测试：
``` python
from ultralytics import YOLO 
model = YOLO("yolov8n.pt")
```

---
### 1.3 封装为 ROS2 节点

发布话题：
``` markdown
/detections
- bbox         # bbox = [x, y, width, height]
- confidence   # 置信度
- class_id     # 类别
```

---
## 2.  目标跟踪（锁定“指定的人”）

---
## 2.1 问题

- 画面中 **不止一个人**
- 要“锁定某一个”
- 多种特殊情况

---
### 2.2 架构

```
┌────────────┐
│ RGB Camera │
└─────┬──────┘
      │
      ▼
┌────────────┐
│   YOLOv8   │─── person bbox + confidence
└─────┬──────┘
      │（只保留被选中的人）
      ▼
┌────────────--┐
│ Depth Camera │─── depth image（与RGB对齐）
└─────┬─────--─┘
      │
      ▼
┌────────────┐
│ 3D Measure │─── (X,Y,Z)
└─────┬──────┘
      │
      ▼
┌────────────┐
│ 3D Kalman  │─── (X,Y,Z,Vx,Vy,Vz)
└─────┬──────┘
      │
      ▼
┌────────────┐
│ Controller │─── 跟随 / 避障 / 规划
└────────────┘

```

#### 2.2.1 从 2D bbox 得到稳定 3D 测量
1. **RGB-D 对齐**
	- depth_image 与 rgb_image 像素对齐的
	- RealSense / Orbbec 都支持硬件或 SDK 对齐

2. **深度聚类**
	- bbox 内取所有有效深度点
	- 用 DBSCAN / KMeans
	- 选**最近的一簇**
	- 计算簇中心

3. **反投影到 3D**
用相机内参：
X=(u−cx)∗Z/fxY=(v−cy)∗Z/fyZ=depth
得到**相机坐标系下的人体中心**

4. **3D 卡尔曼滤波**

5. **目标丢失**
```
                  Tracking
       ↓                             ↓
   Lost (短暂)                    Lost ()
#人物被阻挡，转身等情况             #人物跟丢
   t < 阈值时间                   t > 阈值时间
       ↓                             ↓
只用 Kalman 预测           ReID 特征比较 + Kalman 预测
                      ↓  
                重新初始化Kalman
```

---
### 2.3 目标选择

**通过点击画面选择**

选择目标
``` bash
ros2 topic pub /select_target std_msgs/msg/Int32 "data: ..."
```

---
#### Mode1 ：未选择
只启用yolo的识别相关功能

#### Mode2 ：已选择
1. **系统立刻冻结当前帧**
确定“选的是哪一帧的哪个人”**

2. **从这一帧生成“目标档案”**
这一步是**一次性操作**：
- 提取 ReID embedding
```
ref_embedding = ReID(selected_bbox)
```
- 计算 3D 初始位置
```
(X0, Y0, Z0) = bbox + depth
```
- 初始化 Kalman
```
x0 = [X0, Y0, Z0, 0, 0, 0]
```

---
## 3. 跟随控制

---
### 3.1 控制逻辑

```
#控制节点
订阅
/target_state (Kalman输出)
输出：
/cmd_vel
- linear.x    （前后）
- angular.z   （左右转）
```

---
### 3.2 距离控制
1. **期望跟随距离**
	设定一个期望距离

2. **距离误差**
	ez=Z−Zrefe_z = Z - Z_ref ez​=Z−Zr​ef

3. **线速度控制律（比例控制）**
	v=Kz⋅ezv = K_z · e_z v=Kz​⋅ez​
	并做限幅：
	`v = clamp(v, v_min, v_max)`

4. **距离死区**
	为避免小幅抖动，引入死区：
	`if |e_z| < 0.1 m:     v = 0`
	
---
### 3.3 方位控制
1. **目标相对角度**
	θ=atan2(X,Z)θ = atan2(X, Z) θ=atan2(X,Z)

2. **角速度控制律**
	ω=Kθ⋅θω = K_θ · θ ω=Kθ​⋅θ
	并做限幅：
	`ω = clamp(ω, -ω_max, ω_max)`

3. **小角度抑制（防抖**）
	`if |θ| < 3°:     ω = 0`

#### 流程
```
1. 从 Kalman 读取 (X, Z)
2. 计算 θ = atan2(X, Z)
3. 计算距离误差 e_z = Z - Z_ref
4. 计算线速度 v
5. 计算角速度 ω
6. 限幅 + 死区处理
7. 输出 cmd_vel
```

---
### 3.4 感知异常情况下的控制策略
#### 1. 短时丢失目标（Kalman 预测中）
- 继续使用预测位置行进
- **逐渐降低线速度**
- 保持角速度可用

#### 2. 目标确认丢失（进入 REID / IDLE）
- 立即停止线速度
- 角速度归零
`cmd_vel = (0, 0)`

---
#  局部避障
## 导航与跟随的基本逻辑相同，分2套参数

---

## 1. 静态障碍
1. **静态障碍的来源**
	主要来自：
	- 激光雷达长期稳定点
	- 地图（若有）
	- 深度点云中“长期不动结构”
**不来自超声波**（超声波不区分静/动）

2. **静态障碍的核心目标**
**不撞，也不贴得太近**
	不是“看到就停”，而是：
		- 允许继续跟随目标
		- 同时与障碍保持最小安全距离
		- 跟随目标才是主任务
		- 转向由跟随控制 / Nav 决定
	静态障碍只约束“能走多快”，不决定“往哪走”

## 1.1 静态障碍的处理方式：速度约束而非制动
**定义最小安全距离：**
```
d_static_safe = 0.4 ~ 0.6 m
```

---
**当机器人前方或侧前方存在静态障碍时：**
### 情况 A：距离足够
```
d_static > d_static_safe → 不干预
```

### 情况 B：接近但未危险
```
d_static_min < d_static ≤ d_static_safe
```

行为：
```
限制最大线速度： v_max = k · (d_static - d_static_min)
```

效果：
- 机器人自然慢下来
- 不会贴墙前进
- 不会突然刹停

### 情况 C：极近（异常情况）
```
d_static ≤ d_static_min   (~0.25 m)
```

行为：
```
停止
```

---
## 2. 动态障碍
- 行人（非跟随目标）
- 宠物
- 移动机器人
- 突然出现的物体
**来源：**
- 激光点云的短时变化
- 深度相机的前景运动
- 超声波突发变化（近场）

---
### 2.1动态障碍的核心目标
> **防碰撞优先于跟随**

动态障碍**不要求保持距离**，而是：
- 能停就停
- 能慢就慢

---
### 2.2 动态障碍的分级反应
定义警示距离与停止距离：
```
d_dyn_warn = 0.6 m d_dyn_stop = 0.35 m
```

### 动态障碍 – 减速区
条件：
```
d_dyn_stop < d_dyn ≤ d_dyn_warn
```

行为：
```
v = γ · v   (γ ≈ 0.3 ~ 0.6)
```
- 角速度可保留
- 准备随时停车

---

### 动态障碍 – 停止区
条件：
```
d_dyn ≤ d_dyn_stop
```

行为：
```
cmd_vel = (0, 0)
```

---
## 3. 区分静态 / 动态障碍
**简化可行方案**

| 判据          | 结果     |
| ----------- | ------ |
| 在连续 N 帧位置稳定 | 静态     |
| 距离/角度快速变化   | 动态     |
| 超声波突发       | 动态（默认） |

---


